# -*- coding: utf-8 -*-
"""[DS200] Final-1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/19E6JUVdEN9wSGX9ICnjUttJL6jJoqIwy

# **1. Import**
"""

!pip install -U snorkel

!pip install luminol

!pip install rrcf

!git clone https://github.com/xchuwenbo/realseries.git

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import copy

import torch
from sklearn.metrics import average_precision_score

from snorkel.labeling.model import MajorityLabelVoter, LabelModel

from sklearn.model_selection import train_test_split

import warnings
warnings.filterwarnings("ignore")

from sklearn.decomposition import PCA
import realseries
from realseries.realseries.models.iforest import IForest
from realseries.realseries.models.sr import SpectralResidual
#from realseries.realseries.models.stl import STL
from realseries.realseries.models.lumino import Lumino
from realseries.realseries.models.rcforest import RCForest
from sklearn.svm import OneClassSVM

from google.colab import drive
drive.mount('/content/drive')

path = '/content/drive/MyDrive/Học kì 6/Đồ án/DS200/Dataset/'
data = pd.read_csv(path+'Data_processed.csv')

"""# **4. Model**

## 4.1. **Hàm**
"""

def pro(data):
  data['Date'] = pd.to_datetime(data['Date'])
  #data = data.sort_values('Date')
  data = data.set_index('Date')
  return data

def load_data(path):
    train_data = pro(pd.read_csv(path + "train_data.csv"))
    test_data = pro(pd.read_csv(path + "test_data.csv"))
    train_labels = pd.read_csv(path + "train_labels.csv")
    test_labels = pd.read_csv(path + "test_labels.csv")
    return train_data, test_data, train_labels, test_labels

def evaluate_model(model, X_test, y_test, labels):
    # Dự đoán nhãn của tập kiểm tra
    y_pred = model.predict(X_test)

    # Tính toán các thông số đánh giá
    accuracy = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred, average='weighted')
    recall = recall_score(y_test, y_pred, average='weighted')
    f1 = f1_score(y_test, y_pred, average='weighted')

    # In ra các thông số đánh giá
    print("Accuracy:", accuracy)
    print("Precision:", precision)
    print("Recall:", recall)
    print("F1-score:", f1)

    # In ra báo cáo phân lớp
    print(classification_report(y_test, y_pred, target_names=labels))

    # Tính toán precision, recall và f1-score cho từng lớp
    precision = precision_score(y_test, y_pred, average=None)
    recall = recall_score(y_test, y_pred, average=None)
    f1 = f1_score(y_test, y_pred, average=None)

    # In ra precision, recall và f1-score của từng lớp
    print("\nPrecision, Recall, F1-score for each class:\n")
    for i in range(len(labels)):
        print(labels[i])
        print("Precision:", precision[i])
        print("Recall:", recall[i])
        print("F1-score:", f1[i])
        print()



    # Tạo ma trận nhầm lẫn
    cm = confusion_matrix(y_test, y_pred)

    # Vẽ ma trận nhầm lẫn
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=labels, yticklabels=labels)
    plt.title('Confusion Matrix')
    plt.xlabel('Predicted')
    plt.ylabel('Actual')
    plt.show()

def evaluate_model_cnn(model, X_test, y_test,labels):
    # Dự đoán nhãn của tập kiểm tra
    y_pred = model.predict(X_test)
    y_pred = np.argmax(y_pred, axis=1)

    # Tính toán các thông số đánh giá
    accuracy = np.mean(y_pred == np.argmax(y_test, axis=1))
    report = classification_report(np.argmax(y_test, axis=1), y_pred, target_names=labels)
    confusion = confusion_matrix(np.argmax(y_test, axis=1), y_pred)

    # In ra các thông số đánh giá
    print("Accuracy:", accuracy)
    print("\nClassification Report:\n", report)
    print("\nPrecision, Recall, F1-score:\n")
    precision = precision_score(np.argmax(y_test, axis=1), y_pred, average=None)
    recall = recall_score(np.argmax(y_test, axis=1), y_pred, average=None)
    f1 = f1_score(np.argmax(y_test, axis=1), y_pred, average=None)
    for i in range(len(labels)):
        print(labels[i])
        print("Precision:", precision[i])
        print("Recall:", recall[i])
        print("F1-score:", f1[i])
        print()

    # Tạo ma trận nhầm lẫn
    plt.figure(figsize=(8, 6))
    sns.heatmap(confusion, annot=True, cmap='Blues', fmt='g', xticklabels=labels, yticklabels=labels)
    plt.xlabel('Predicted')
    plt.ylabel('True')
    plt.title('Confusion Matrix')
    plt.show()

"""## Load data"""

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.svm import OneClassSVM
from sklearn.ensemble import IsolationForest, RandomForestClassifier, GradientBoostingClassifier
from sklearn.neighbors import KNeighborsClassifier
import xgboost as xgb

from keras.models import Sequential
from keras.layers import Dense, Activation, Dropout
import tensorflow as tf
import numpy as np
from sklearn.datasets import make_classification
from imblearn.over_sampling import SMOTE
from sklearn.svm import SVC

from sklearn import metrics
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report
from sklearn.metrics import confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns


import warnings
warnings.filterwarnings("ignore")

from google.colab import drive
drive.mount('/content/drive')

"""
X = data.drop('Label', axis=1)
y = data['Label']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)

path = '/content/drive/MyDrive/Học kì 6/Đồ án/DS200/Dataset/'
# Save
X_train.to_csv(path+"train_data.csv")
y_train.to_csv(path+"train_labels.csv")
X_test.to_csv(path+"test_data.csv")
y_test.to_csv(path+"test_labels.csv")
"""

path = '/content/drive/MyDrive/Học kì 6/DS200/Dataset/Split/'
X_train, X_test, y_train, y_test = load_data(path)

len(y_train)

y_test.value_counts()

"""## 4.2. **Trước khi xử lý cân bằng**

### 4.2.1. *Xây dựng và fit model*
"""

# 1. SVM
svm_model = SVC(kernel='rbf')
svm_model.fit(X_train,y_train)

# 2. XGBoost
xgb_model = xgb.XGBClassifier(objective="binary:logistic", random_state=42)
xgb_model.fit(X_train, y_train)

# 3. Random Forest
rf_model = RandomForestClassifier()
rf_model.fit(X_train, y_train)

# 4. Gradient Boosting Machine
gbm_model = GradientBoostingClassifier()
gbm_model.fit(X_train, y_train)

# 5. K-nearest neighbors (KNN)
knn_model = KNeighborsClassifier()
knn_model.fit(X_train, y_train)

"""### 4.2.2. *Đánh giá mô hình*"""

labels =['Normal','Anomaly']

print("SVM:")
evaluate_model(svm_model, X_test, y_test, labels)

print("xgboost:")
evaluate_model(xgb_model, X_test, y_test, labels)

print("Random Forest:")
evaluate_model(rf_model, X_test, y_test, labels)

print("Gradient Boosting Machine:")
evaluate_model(gbm_model, X_test, y_test, labels)

print("K-nearest neighbors (KNN):")
evaluate_model(knn_model, X_test, y_test, labels)

"""### 4.2.3. *Lưu model*"""

import joblib

# save
path_model = '/content/drive/MyDrive/Học kì 6/DS200/Model/'

joblib.dump(svm_model, path_model + "svm_model.pkl")
xgb_model.save_model(path_model + "xgb_model.h5")
joblib.dump(rf_model, path_model + "rf_model.pkl")
joblib.dump(gbm_model, path_model + "gbm_model.pkl")
joblib.dump(knn_model, path_model + "knn_model.pkl")

"""## 4.3. **Sau khi xử lý cân bằng**"""

# In số lượng mẫu của từng lớp trước khi áp dụng SMOTE
print("Số lượng mẫu huấn luyện trước khi SMOTE:")
print(y_train.value_counts())

# Áp dụng SMOTE để điều chỉnh mẫu trên tập huấn luyện
smote = SMOTE(sampling_strategy='auto', random_state=42)
X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)

# In số lượng mẫu của từng lớp sau khi áp dụng SMOTE
print("Số lượng mẫu huấn luyện sau khi SMOTE:")
print(y_train_resampled.value_counts())

"""### 4.3.1. *Xây dựng và fit model*"""

# 1. SVM
svm_model_1 = SVC(kernel='rbf')
svm_model_1.fit(X_train_resampled,y_train_resampled)

# 2. XGBoost
xgb_model_1 = xgb.XGBClassifier(objective="binary:logistic", random_state=42)
xgb_model_1.fit(X_train_resampled, y_train_resampled)

# 3. Random Forest
rf_model_1 = RandomForestClassifier()
rf_model_1.fit(X_train_resampled, y_train_resampled)

# 4. Gradient Boosting Machine
gbm_model_1 = GradientBoostingClassifier()
gbm_model_1.fit(X_train_resampled, y_train_resampled)

# 5. K-nearest neighbors (KNN)
knn_model_1 = KNeighborsClassifier()
knn_model_1.fit(X_train_resampled, y_train_resampled)

"""### 4.3.2. *Đánh giá mô hình*"""

labels =['Normal','Anomaly']

print("SVM:")
evaluate_model(svm_model_1, X_test, y_test, labels)

print("xgboost:")
evaluate_model(xgb_model_1, X_test, y_test, labels)

print("Random Forest:")
evaluate_model(rf_model_1, X_test, y_test, labels)

print("Gradient Boosting Machine:")
evaluate_model(gbm_model_1, X_test, y_test, labels)

print("K-nearest neighbors (KNN):")
evaluate_model(knn_model_1, X_test, y_test, labels)

"""### 4.3.3. *Lưu model*"""

# save
path_model = '/content/drive/MyDrive/Học kì 6/DS200/Model/'

joblib.dump(svm_model_1, path_model + "svm_model_sampling.pkl")
xgb_model_1.save_model(path_model + "xgb_model_sampling.h5")
joblib.dump(rf_model_1, path_model + "rf_model_sampling.pkl")
joblib.dump(gbm_model_1, path_model + "gbm_model_sampling.pkl")
joblib.dump(knn_model_1, path_model + "knn_model_sampling.pkl")